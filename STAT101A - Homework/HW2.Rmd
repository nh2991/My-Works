---
output:
  pdf_document: default
  html_document: default
---

---
title: \textbf{Homework 2}
author: "Juwon Lee, Economics and Statistics, UCLA"
date: "2023-01-13"
output: 
  pdf_document:
    latex_engine: xelatex
---

tinytex::install_tinytex()

## 1.

\noindent \textcolor{red}{\textbf{(a)}} Show that the sum of residuals is always zero, i.e. $\sum^n_{i=1} \hat{e_i} = 0$. 

\noindent Claim $\sum^n_{i=1} \hat{e_i} = 0$, having $E(\hat{\beta_0}) = \beta_0,~E(\hat{\beta_1}) = \beta_1,~E(e_i) = 0$.

\noindent $\sum^n_{i=1} \hat{e_i} = \sum^n_{i=1} (y_i - \hat{y_i}) = \sum^n_{i=1} [ (\beta_0 + \beta_1 x_i + e_i) - (\hat{\beta_0} + \hat{\beta_1} x_i)] = \sum^n_{i=1} (\beta_0 - \hat{\beta_0}) + \sum^n_{i=1} (\beta_1 - \hat{\beta_1}) x_i + \sum^n_{i=1} e_i$.

\noindent Because $E(\hat{\beta_0}) = \frac{1}{n} \sum^n_{i=1} \hat{\beta_0} = \beta_0$, $\sum^n_{i=1} \hat{\beta_0} = n \beta_0 = \sum^n_{i=1} \beta_0$, so that $\sum^n_{i=1} (\beta_0 - \hat{\beta_0}) = 0$.

\noindent And $E(\hat{\beta_1}) = \frac{1}{n} \sum^n_{i=1} \hat{\beta_1} = \beta_1$, $\sum^n_{i=1} (\hat{\beta_1} - \beta_1) = 0$.

\noindent Thus, $\sum^n_{i=1} \hat{e_i} = \sum^n_{i=1} e_i$.

\noindent Because $E(e_i) = \frac{1}{n} \sum^n_{i=1} e_i = 0$, so that $\sum^n_{i=1} e_i = 0$. QED

\vspace{4mm}

\noindent \textcolor{red}{\textbf{(b)}} Show that $\hat{\beta_0}$ and $\hat{\beta_1}$ are the least square estimates, i.e. $\hat{\beta_0}$ and $\hat{\beta_1}$ minimizes $\sum \hat{e}^2$.

\noindent $\sum^n_{i=1} \hat{e_i}^2 = \sum^n_{i=1} (y_i - \hat{y_i})^2 = \sum^n_{i=1} (y_i - b_0 - b_1 x_i)^2 = \sum^n_{i=1} \{ (\beta_0 + \beta_1 x_i) - (b_0 + b_1 x_i)\}^2$,

\noindent claim that $\hat{\beta_0}$ and $\hat{\beta_1}$ minimize $\sum^n_{i=1} \hat{e_i}^2$.

\noindent $\rightarrow \min_{b_0,~b_1} \sum^n_{i=1} \{ (\beta_0 + \beta_1 x_i) - (b_0 + b_1 x_i) \}^2 = \min_{b_0,~b_1} \sum^n_{i=1} \{ (\beta_0 - b_0) + (\beta_1 - b_1 ) x_i\}^2$

\noindent $\rightarrow b_0 = \hat{\beta_0},~b_1 = \hat{\beta_1}$. QED

\vspace{4mm}

\noindent \textcolor{red}{\textbf{(c)}} Show that $S^2$ is an unbiased estimator of $\sigma^2$.

\noindent Claim $E(S^2) = \sigma^2$ such that $S^2 = \frac{1}{n-2} \sum^n_{i=1} \hat{e_i}^2$.

\noindent $E(S^2) = E[ \frac{1}{n-2} \sum^n_{i=1} \hat{e_i}^2]$ $= E[\frac{1}{n-2} \sum^n_{i=1} (y_i - \hat{y_i})^2]$ $= \frac{1}{n-2} \sum^n_{i=1} E[(y_i - \hat{y_i})^2]$

\noindent $= \frac{1}{n-2} \sum^n_{i=1} [E(y_i^2) + E(\hat{y_i}^2) - 2 E(y_i \hat{y_i})]$

\noindent $= \frac{1}{n-2} \sum^n_{i=1} \{[E(y_i)^2 - (E(y_i))^2] + (E(y_i))^2 + [E(\hat{y_i})^2 - (E(\hat{y_i}))^2 + (E(\hat{y_i}))^2] - 2E(y_i \hat{y_i}) \}$

\noindent = $\frac{1}{n-2} \sum^n_{i=1} [ V(y_i) + (E(y_i))^2 + V(\hat{y_i}) + (E(\hat{y_i}))^2 - 2 E(y_i \hat{y_i})]$

\noindent $= \frac{1}{n-2} \sum^n_{i=1} [ \sigma^2 + (E(y_i))^2 + 0 + (E(\hat{y_i}))^2 - 2E(y_i \hat{y_i})]$

\noindent $=\frac{1}{n-2} \sum^n_{i=1} [\sigma^2 + 2(E(y_i))^2 - 2E(y_i \hat{y_i})]=\frac{1}{n-2} \sum^n_{i=1} [\sigma^2 - 2 (E(y_i \hat{y_i}) - (E(y_i))^2)]$

\noindent $=\frac{1}{n-2} \sum^n_{i=1} \sigma^2 - \frac{2}{n-2} \sum^n_{i=1} [E(y_i \hat{y_i}) - (E(y_i))^2]$

\noindent $=\frac{n \sigma^2}{n-2} - \frac{2\sigma^2}{n-2} = \frac{(n-2) \sigma^2}{n-2} = \sigma^2$. QED

\newpage

## 2.

```{r}
indicators <- read.table('indicators.txt', header=T)
indicators_lm <- indicators[c(2,3)]

PriceChange <- as.vector(indicators_lm[1])
LoanPaymentsOverdue <- as.vector(indicators_lm[2])

data_lm_hw2_2 <- lm(PriceChange~LoanPaymentsOverdue, data=indicators)
data_lm_hw2_2

summary(data_lm_hw2_2)

Sxx_hw2_2 <- colSums((indicators_lm[2] - colMeans(indicators_lm[2]))^2)

len_hw2_2 <- length(indicators_lm$PriceChange)

Syy_hw2_2 <- colSums((indicators_lm[1] - colMeans(indicators_lm[1]))^2)
s_hw2_2 <- (sum(data_lm_hw2_2$residuals^(2)) / (len_hw2_2-2))^(1/2)
se_1_hw2_2 <- s_hw2_2 / (Sxx_hw2_2)^(1/2)

data_lm_hw2_2$coefficients[2] - qt(0.975, len_hw2_2-2) * se_1_hw2_2
data_lm_hw2_2$coefficients[2] + qt(0.975, len_hw2_2-2) * se_1_hw2_2

```
\noindent \textcolor{red}{\textbf{(a)}} Thus, the 95\% confidence interval is (-4.163454, -0.333585). If $H_0:~\beta_1 > 0,~H_1:~\beta_1 <0$, then the $p$-value = 0.02419 < 0.05, so that we can reject the null. It means that we can't say that $\beta_1 > 0$.

\vspace{4mm}
\noindent \textcolor{red}{\textbf{(b)}} $E(Y|X=4) = 4.514 - 2.249 * 4$

```{r}
data_lm_hw2_2$coefficients[1] + data_lm_hw2_2$coefficients[2] * 4
```

\noindent Thus, $E(Y|X=4) = -4.479585.

\vspace{4mm}
\noindent If we take the interval estimation,

```{r}

E_hw2_2 <- (data_lm_hw2_2$coefficients[1] + data_lm_hw2_2$coefficients[2] * 4)
barx_hw2_2 <- colMeans(indicators_lm[2])

E_hw2_2 - qt(0.975, len_hw2_2-2) * s_hw2_2 * (1/len_hw2_2 + ((4-barx_hw2_2)^2 / Sxx_hw2_2))^(1/2) 

E_hw2_2 + qt(0.975, len_hw2_2-2) * s_hw2_2 * (1/len_hw2_2 + ((4-barx_hw2_2)^2 / Sxx_hw2_2))^(1/2) 

data_hw2_2 = data.frame(LoanPaymentsOverdue=4)
predict(data_lm_hw2_2, newdata=data_hw2_2, interval='confidence', level=0.95)
```

\noindent Thus, the 95\% confidence interval for $E(Y|X=4)$ is (-6.648849, -2.310322). It means that 0\% is not a feasible value for $E(Y|X=4)$ for $\alpha = 0.05$.

\newpage

## 3.

```{r}
invoices <- read.table('invoices.txt', header=T)

invoices_lm <- invoices[c(2,3)]

data_lm_hw2_3 <- lm(Time~Invoices, data=invoices)
data_lm_hw2_3

summary(data_lm_hw2_3)

barx_hw2_3 <- colMeans(invoices_lm[1])

Sxx_hw2_3 <- colSums((invoices_lm[1] - colMeans(invoices_lm[1]))^2)

len_hw2_3 <- length(invoices$Invoices)

Syy_hw2_3 <- colSums((invoices_lm[2] - colMeans(invoices_lm[2]))^2)
s_hw2_3 <- (sum((data_lm_hw2_3$residuals)**2) / (len_hw2_3-2))^(1/2)

se_0_hw2_3 <- sd(data_lm_hw2_3$residuals) * ((1/len_hw2_3) + ((barx_hw2_3)^2 / Sxx_hw2_3))^(1/2)

data_lm_hw2_3$coefficients[1] - qt(0.975, len_hw2_3-2) * se_0_hw2_3
data_lm_hw2_3$coefficients[1] + qt(0.975, len_hw2_3-2) * se_0_hw2_3


```

\noindent \textcolor{red}{\textbf{(a)}} Thus, the 95\% confidence level is $(0.3956058, 0.887814)$.

\vspace{4mm}
\noindent \textcolor{red}{\textbf{(b)}} $H_0 : \beta_1 = 0.01$ vs. $H_1 : \beta_1 \neq 0.01$.  

\noindent Then

```{r}

se_1_hw2_3 <- s_hw2_3 / (Sxx_hw2_3)^(1/2)
se_1_hw2_3

Statistic_hw2_3 <- (data_lm_hw2_3$coefficients[2] - 0.01) / se_1_hw2_3
Statistic_hw2_3

qt(0.975, len_hw2_3-2)

```
\noindent Thus, the statistic 1.578251 < 2.048407, so we cannot reject the null. Then, we can't say that $\beta_1$ is not 0.01.

\vspace{4mm}
\noindent \textcolor{red}{\textbf{(c)}} Suppose that $x = 130$. Then $Y|(X=130) = 0.64171 + 0.01129 * 130 = 2.109624$.

```{r}
data_lm_hw2_3
invoices130_hw2_3 <- data_lm_hw2_3$coefficients[1] + data_lm_hw2_3$coefficients[2] * 130
se130_hw2_3 <- s_hw2_3 * (1 + 1/len_hw2_3 + (130-barx_hw2_3)^2/Sxx_hw2_3)^(1/2)

invoices130_hw2_3 - qt(0.975, len_hw2_3-2) * se130_hw2_3
invoices130_hw2_3 + qt(0.975, len_hw2_3-2) * se130_hw2_3

data_hw2_3 <- data.frame(Invoices=130)
predict(data_lm_hw2_3, newdata=data_hw2_3, interval='prediction', level=0.95)

```

\noindent so the 95\% prediction interval is (1.422947, 2.7963).

\newpage

## 4.

\noindent \textcolor{red}{\textbf{(a)}} Claim $(y_i - \hat{y_i}) = (y_i - \bar{y}) - \hat{\beta_1} (x_i - \bar{x})$.

\noindent $\rightarrow-(\hat{\beta_0} + \hat{\beta_1} x_i) = - \bar{y} - \hat{\beta_1} x_i + \hat{\beta_1} \bar{x}$.

\noindent $\rightarrow~-\hat{\beta_0} = - \bar{y} + \hat{\beta_1} \bar{x}$.

\noindent $\rightarrow~\bar{y} = \hat{\beta_0} + \hat{\beta_1} \bar{x}$.

\noindent $\leftrightarrow~E(Y) = \hat{\beta_0} + \hat{\beta_1} E(X) = \hat{\beta_0} +\hat{\beta_1} \bar{x}$. QED

\vspace{4mm}
\noindent \textcolor{red}{\textbf{(b)}} Claim $(\hat{y_i} - \bar{y}) = \hat{\beta_1} (x_i - \bar{x})$.

\noindent $\rightarrow~(\hat{\beta_0} + \hat{\beta_1} x_i - \bar{y}) = \hat{\beta_1} x_i - \hat{\beta_1} \bar{x}$.

\noindent $\rightarrow~\bar{y} = \hat{\beta_0} + \hat{\beta_1} \bar{x}$, which is same with (a). QED

\vspace{4mm}
\noindent \textcolor{red}{\textbf{(c)}} Claim $\sum^n_{i=1} (y_i - \hat{y_i})(\hat{y_i} - \bar{y}) = 0$, using $\hat{\beta_1} = \frac{Sxy}{Sxx} = \frac{\sum^n_{i=1} (x_i - \bar{x})(y_i - \bar{y})}{\sum^n_{i=1} (x_i -\bar{x})^2}$.

\noindent $\rightarrow~\sum^n_{i=1} (y_i - (\hat{\beta_0} + \hat{\beta_1} x_i))((\hat{\beta_0} + \hat{\beta_1} x_i) - \bar{y}) = 0$

\noindent $\rightarrow~\sum^n_{i=1} (y_i - (\hat{\beta_0} + \hat{\beta_1} x_i)) ((\hat{\beta_0} + \hat{\beta_1} x_i) - (\hat{\beta_0} + \hat{\beta_1} \bar{x})) = 0$

\noindent $\rightarrow~ \sum^n_{i=1} (y_i - (\hat{\beta_0} + \hat{\beta_1} x_i)) * \hat{\beta_1} (x_i - \bar{x}) = 0$

\noindent thus, if $\sum^n_{i=1} (x_i - \bar{x})=0$, then it is clear.

\noindent $\rightarrow~\sum^n_{i=1} x_i - n \bar{x} =0,~\bar{x} = \frac{1}{n} \sum^n_{i=1} x_i$. QED

## 5.

\noindent $X$ is the distance, $Y$ is airfares.  
And len($Y$) = 17, E($Y$) = $\frac{1}{n} \sum^n_{i=1} y_i$ = 228.35, sd($Y$)= $\sqrt{\frac{1}{n-1} \sum^n_{i=1} (y_i - \bar{y})^2}$ = 129.74,

\noindent E($X$) =$\frac{1}{n} \sum^n_{i=1} x_i$= 816.53, sd($X$) =$\sqrt{\frac{1}{n-1} \sum^n_{i=1} (x_i - \bar{x})^2}$ = 588.79.

\noindent Moreover, $S=10.41,~\hat{\beta_0} = 48.97177,~\hat{\beta_1} = 0.219687$.

\vspace{4mm}
\noindent \textcolor{red}{\textbf{(a)}} First of all, because $Sxx = \sum^n_{i=1} (x_i - \bar{x})^2 = (588.79)^2 * 16 = 5546799$  
and $se(\hat{\beta_0}) = s \sqrt{(\frac{1}{n} + \frac{\bar{x}^2}{Sxx})}$ and $se(\hat{\beta_1}) = \frac{s}{\sqrt{Sxx}}$.

```{r}
Sxx_hw2_5 <- (588.79)^2 * 16
Sxx_hw2_5

(10.41) * ((1/17) + (816.53)^2 / Sxx_hw2_5)^(1/2)
(10.41) / (Sxx_hw2_5)^(1/2)
```

\noindent Thus, (1) 4.40459, (4) 0.004420082.

```{r}
48.97177 / ((10.41) * ((1/17) + (816.53)^2 / Sxx_hw2_5)^(1/2))
0.219687 / ((10.41) / (Sxx_hw2_5)^(1/2))
```

\noindent Then, (2) 11.11835, (5) 49.70202.

```{r}
2*(1-pt(11.11835, 15))
2*(1-pt(49.70202, 15))
```

\noindent Then, (3) 1.217648e-08, (6) 0.

\vspace{4mm}
\noindent And because Adjusted R-squared 
$= 1 - \frac{n-1}{n-k-1} (1-R^2) = 1 - \frac{16}{15} (1-R^2) = 0.9936$, $(\because~k=1)$,  
so that $R^2 = 1 - (1-0.9936) * \frac{15}{16} = 0.994$.

```{r}
1 - (1 - 0.9936) * 15 / 16
```

\noindent Thus, (7) 0.994.

\noindent Finally, we can say that
\noindent \begin{tabular}{c|c|c|c|c}
& Estimate & Std. Error & $t$ value & $Pr(>|t|)$ \\
\hline (intercept) & 48.971770 & 4.40459 & 11.11835 & 1.217648e-08 \\
Distance & 0.219687 & 0.004420082 & 49.70202 & $\approx$ 0 \\
\hline Multiple R-squared & 0.994 \\
\end{tabular}

\vspace{4mm}
\noindent And, because we have known that the $F$-statistic is 2469 and $p$-value is $2.2e-16$, so

\noindent \begin{tabular}{c|c|c|c|c|c}
& Df & Sum Sq & Mean Sq & $F$ value & Pr(>F) \\
\hline Distance & 1 & SSA & SSA & 2469 & $2.2e-16$ \\
Residuals & 15 & SSE & SSE/15 \\
\end{tabular}

\vspace{4mm}
\noindent Also, $\frac{1}{n} \sum^n_{i=1} y_i = 228.35$ and $\frac{1}{n-1} \sum^n_{i=1} (y_i - \bar{y})^2 = (129.74)^2$,

\noindent $\rightarrow~\sum^n_{i=1} y_i^2 - 228.35 * 2 \sum^n_{i=1} y_i + 17*(228.35)^2 = (129.34)^2 * 16$,

\noindent $\rightarrow~\sum^n_{i=1} y_i^2 = 228.35 * 2 \sum^n_{i=1} y_i+ (129.34)^2 *16 - 17*(228.35)^2 = 1155763$

\noindent then, $CT = \frac{T^2}{N} = \frac{(17*228.35)^2}{17} = 886443.3$

\noindent Thus, $SST = \sum^n_{i=1} y_i^2 - CT = 269319.5$.

```{r}
228.35 * 2 * 228.35 * 17 + (129.74)^2 * 16 - 17 * (228.35)^2

(17*228.35)^2 / 17

228.35 * 2 * 228.35 * 17 + (129.74)^2 * 16 - 17 * (228.35)^2 - (17*228.35)^2 / 17
```

\noindent Now, we have 
\noindent $\begin{cases} \frac{SSA}{SSE/15} = 2469 \\ 269319.5 = SST = SSA + SSE \\ \end{cases}$

\noindent $\rightarrow~15 *SSA = 2469 *SSE,~15*SSA + 15*SSE = 269319.5 * 15$   
$\rightarrow~2484 * SSE = 269319.5 * 15,~\therefore~SSE = 1626.325$  
$\rightarrow~SSA = 2469 * SSE / 15 = 267693.2$.

\noindent Finally, we can conclude that

\noindent \begin{tabular}{c|c|c|c|c|c}
& Df & Sum Sq & Mean Sq & $F$ value & Pr(>F) \\
\hline Distance & 1 & 267693.2 & 267693.2 & 2469.001 & $2.2e-16$ \\
Residuals & 15 & 1626.325 & 108.4217 \\
\end{tabular}

```{r}
269319.5 * 15 / 2484
2469 * (269319.5 * 15 / 2484) / 15
1626.325 / 15
267693.2 / (1626.325 / 15)
```

\noindent \textcolor{red}{\textbf{(b)}} $y = 48.97177 + 0.219687 * x$.

\vspace{4mm}
\noindent \textcolor{red}{\textbf{(c)}} $H_0 : \beta_1 =0$ vs $H_1 : \beta_1 \neq 0$.

\noindent $T = \frac{0.219687}{0.1212024} = 1.812563$,

\noindent $P(|t| < 1.812563) = 0.08996026 > 0.025$, so we can't reject the null.

\noindent Thus, we can't say that $\beta_1$ is not zero.

```{r}
0.219687 / 0.1212024
2 * (1 - pt(1.812563, 15))
```

\noindent For $\beta_0,~H_0 : \beta_0 = 0$ vs $H_1 : \beta_0 \neq 0$.

\noindent $T = \frac{48.971770}{99.00649} = 0.4946319$,

\noindent $P(|t| < 0.4946319) = 0.6280259 > 0.025$, so we can't reject the null.

\noindent Thus, we can't say that $\beta_0$ is not zero.

```{r}
48.971770 / 99.00649
2 * (1 - pt(0.4946319, 15))
```

\newpage

\noindent \textcolor{red}{\textbf{(d)}} $R^2 = 0.994$ explains that, 99.4\% percentage of sum-of-square of $y$(airfares) is explained by $x$(distance).

\vspace{4mm}
\noindent \textcolor{red}{\textbf{(e)}} $H_0 : \beta_1 = 0$ vs $H_1 : \beta_1 \neq 0$.

```{r}
qf(0.95, 1, 15)
```

\noindent Thus, $F = 2469 > F_{0.05} (1,~15) = 4.543077$, so we can reject the null.

```{r}
qt(0.975, 15)^2
```

\noindent Then, it is not consistent to the hypothesis testing for the slope,  
but $F_{0.05} (1,~15) = (t_{0.025} (15))^2$.